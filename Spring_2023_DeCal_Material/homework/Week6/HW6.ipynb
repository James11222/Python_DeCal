{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6530dad8",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "This week we  learned about numerical differentiation and numerical integration. We will be leaving the Differential Equations until Homework 7, where we delve into animation. All of these skills are extremely useful in any STEM based course. The goal for this homework is to apply what we learned for some problems and see how these tools can be used for our benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b909ef0",
   "metadata": {},
   "source": [
    "# Curve Fitting\n",
    "\n",
    "#### The following problem will be us exploring how to fit a set of data.\n",
    "\n",
    "#### This is inherently very statistics heavy to fully understand. So, do not freak out if the rest of this cell doesn't make any sense to you right now. It freaked me out when I first looked at it but now that I have actually learned stats in my other courses it isn't too scary to read. I am just going to put it here for your reference in the future if you want to understand what is happening more deeply. \n",
    "\n",
    "\n",
    "Note: The following information was provided and adapted from Physics 77\n",
    "\n",
    "\n",
    "The simplest technique to describe is **least-squares fitting**. Usually you use the least-squares fit if **you have a set of data** (pairs of data points $(x_i, y_i)$ ), and **you want to describe it in terms of a model** $y(x;\\{\\theta_j\\})$, where **you have parameters** $\\{\\theta_j\\}$ **that are unknown**. The purpose of your fit is to determine the values of $\\{\\theta_j\\}$ and (hopefully) their uncertainties. An example of a model is:\n",
    "\n",
    "$$y = a_0 + a_1 x$$\n",
    "\n",
    "where the unknown parameters $\\theta_j$ are $a_0$ and $a_1$.\n",
    "\n",
    "There are two standard cases where least-squares method is applicable:\n",
    "1. You know errors for each data point $\\sigma_i$ and you know that those errors are Gaussian. In this case, you minimize $\\chi^2=\\sum \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2$ with respect to the parameters $\\{\\theta_j\\}$. The value of the $\\chi^2_{\\min}$ can be interpreted as a goodness-of-fit. The parameters $\\{\\theta_j\\}$ that minimize $\\chi^2$ have probabilistic interpretation\n",
    "1. You know that the errors are Gaussian and are the same for each data point, but you do not know their magnitude. In this case, you would minimize the sum of squares: $\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\theta)\\right)^2$. Then value of $\\mathcal{S}$ can be used to *estimate* the errors $\\sigma_i$ for each data point: $\\sigma_i = \\sqrt{\\mathcal{S}/(N_\\mathrm{data}-N_\\mathrm{parameters})}$\n",
    "The errors on $\\theta$ have a probabilistic definition, but you lose information about the goodness of fit\n",
    "1. If the errors are not known to be Gaussian, then the least square method is not useful to estimate uncertainties or the goodness of fit. It is also not guaranteed to be unbiased or most efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4cb93",
   "metadata": {},
   "source": [
    "### Let's try it out by fitting a straight line model to some data\n",
    "\n",
    "\n",
    "First lets generate some fake data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as fitter\n",
    "\n",
    "# Generate artificial data = straight line with a=0 and b=1\n",
    "# plus some noise.\n",
    "a0 = 0\n",
    "b0 = 1\n",
    "sig = 0.2\n",
    "Npoints = 10\n",
    "xdata = np.arange(0,Npoints,1.)\n",
    "ydata = a0+xdata*b0+sig*np.random.standard_normal(size=Npoints)\n",
    "sigma = np.ones(Npoints)*sig\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(xdata,ydata,color='b')\n",
    "plt.errorbar(xdata,ydata, sigma, color='r',ls='none')\n",
    "plt.xlim(-1,Npoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you run the cell above this first to generate some artificial data\n",
    "# to which we will fit this curve\n",
    "\n",
    "#=============================================================================================\n",
    "# Define a fit model. For this part, we will use a linear function\n",
    "# The function which defines your model HAS TO TAKE ON A SPECIFIC FORM\n",
    "#=============================================================================================\n",
    "\n",
    "def model(x, a, b):\n",
    "    return a + b*x\n",
    "\n",
    "# You have to supply an initial guess of parameters, and they should be \"close enough\" to the true values, otherwise\n",
    "# the fit may fall into a false minimum\n",
    "par0    = np.array([0.5, -0.3]) # initial guess for parameters\n",
    "par, cov = fitter.curve_fit(model, xdata, ydata, par0, sigma, absolute_sigma=True)\n",
    "\n",
    "#=============================================================================================\n",
    "# the par arrays contains the values of parameters. cov is the covariance matrix\n",
    "# decode it now\n",
    "#=============================================================================================\n",
    "\n",
    "print(\"Your model's parameters and their uncertainties are the following: \\n\")\n",
    "a = par[0]\n",
    "ea = np.sqrt(cov[0,0])\n",
    "print('a={0:6.3f}+/-{1:5.3f} \\n'.format(a,ea))\n",
    "b = par[1]\n",
    "eb = np.sqrt(cov[1,1])\n",
    "print('b={0:6.3f}+/-{1:5.3f} \\n\\n'.format(b,eb))\n",
    "\n",
    "#=============================================================================================\n",
    "# compute reduced chi2\n",
    "#=============================================================================================\n",
    "\n",
    "print(\"Your model's chi^2 value and reduced chi^2 value and their uncertainties are the following: \\n\")\n",
    "chi_squared = np.sum(((ydata - model(xdata, *par))/sigma)**2)\n",
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2 = {0:5.2f} \\n'.format(chi_squared))\n",
    "print ('chi^2/d.f.={0:5.2f} \\n'.format(reduced_chi_squared))\n",
    "\n",
    "#=============================================================================================\n",
    "# overlay plot over data\n",
    "#=============================================================================================\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o', label='data') #plotting the data\n",
    "plt.xlim(-1,Npoints)\n",
    "xfit = np.linspace(0,Npoints-1.,50)\n",
    "plt.plot(xfit,model(xfit,par[0],par[1]),'r-', label='model') #plotting the model\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af331059",
   "metadata": {},
   "source": [
    "In summary:\n",
    "1. for curve fitting you need to write a model function that you think the data should fit based on some unknown parameters\n",
    "1. you need to make an array full of guesses for each unknown parameter\n",
    "1. use `scipy.optimize.curve_fit` function to fit the model's paramters to best match the data\n",
    "1. This function spits out two things, the parameters array which it's length depends on how many parameters your model needs. The second thing is the Covariance 2D array which the square root of the diagonals correspond to the uncertainties of each parameter\n",
    "1. We now can plug these found parameters into our model and plot to see how well the model fits the data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a037c",
   "metadata": {},
   "source": [
    "## Problem 1 (25 Points)\n",
    "\n",
    "We now introduced a new function: `scipy.optimize.curve_fit()`. The code in the cell immediately below will generate some data where the first column is $x$ values, the second column is the $y$ values, the third column is the uncertainty in each value. Use the techniques above to fit a quadratic model of the form\n",
    "$$y = a_0 + a_1 x + a_2 x^2$$\n",
    "\n",
    "Plot the data and your best fit curve with error and print out the values and their uncertainties as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f094fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate artificial data = quadratic function with a0 = .5, a1=1, a2 = -0.3\n",
    "# plus some noise.\n",
    "a0 = 0.5\n",
    "a1 = 1\n",
    "a2 = -0.3\n",
    "sig = 0.4\n",
    "Npoints = 10\n",
    "\n",
    "xdata = np.arange(0,Npoints,1.)\n",
    "ydata = a0 + a1 * xdata + a2 * xdata **2 + sig * np.random.standard_normal(size=Npoints)\n",
    "sigma = np.ones(Npoints)*sig\n",
    "\n",
    "print(\"Your x data values are stored as xdata\")\n",
    "print(\"Your y data values are stored as ydata\")\n",
    "print(\"Your y data uncertainties are stored as sigma\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(xdata,ydata,color='b')\n",
    "plt.errorbar(xdata,ydata, sigma, color='r',ls='none')\n",
    "plt.xlim(-1,Npoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6414d2",
   "metadata": {},
   "source": [
    "**Write your solution for this problem in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb07ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3141bf6",
   "metadata": {},
   "source": [
    "# Root Finding\n",
    "\n",
    "The following problem will be us exploring how to find the roots of a function numerically.\n",
    "\n",
    "## Problem 2 (25 Points)\n",
    "\n",
    "### [Adapted from Newman 6.15]\n",
    "\n",
    "Consider a sixth-order polynomial \n",
    "$$P(x) = 924x^6 - 2772x^5 + 3150x^4 -1680x^3 +420x^2 -42x + 1$$\n",
    "There is no general formula for the roots of a polynomial of degree 6, but you can compute the roots numerically. \n",
    "1. Make a plot of $P(x)$ from $x=0$ to $x=1$ and by inspecting it find rough values for the six roots of the polynomial. \n",
    "1. Write the code to solve for the positions of all six roots to at least ten decimal places using at least one of the methods dsicussed in class. (you can/should use the built-in functions). \n",
    "\n",
    "Hint: I would recommend using `fsolve` in the scipy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "###Your Code Here###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a28cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Preamble#\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24915c",
   "metadata": {},
   "source": [
    "# Example: Numerical Differentiation\n",
    "\n",
    "SciPy offers a library function to compute derivatives. It uses a central difference formula, but with additional ability to use more than two points. Here is an example of its use, it is called `scipy.mis.derivative`. See full documentation at http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.derivative.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345778d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative using order = 3: 0.00000, takes 0.00018 sec to compute using your computer\n",
      "Derivative using order = 101: -0.00000, takes 0.00365 sec to compute using your computer\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import derivative\n",
    "import time\n",
    "\n",
    "# Compute the derivative of cos(x) at x = pi\n",
    "t0 = time.time()\n",
    "dy_scipy_1 = derivative(func=np.cos, x0=np.pi, dx=1e-6, n=1, order=3)\n",
    "t1 = time.time()\n",
    "dy_scipy_2 = derivative(func=np.cos, x0=np.pi, dx=1e-6, n=1, order=101)\n",
    "t2 = time.time()\n",
    "\n",
    "dt1 = t1 - t0\n",
    "dt2 = t2 - t1\n",
    "print(\"Derivative using order = 3: %.5f, takes %.5f sec to compute using your computer\" % (dy_scipy_1, dt1))\n",
    "print(\"Derivative using order = 101: %.5f, takes %.5f sec to compute using your computer\" % (dy_scipy_2, dt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001c6b6",
   "metadata": {},
   "source": [
    "The arguments here are as follows:\n",
    "\n",
    "    func:  the name of the function whose derivative you want to calculate\n",
    "    x0:    the location of the value (scalar) or values (numpy array) where you want to evaluate the derivative\n",
    "    dx:    spacing between the points it generates (by evaluating `func`) to calculate the differences\n",
    "    n:     the number of derivatives. `n=1` means first derivative of `func`\n",
    "    order: number of points to use in the difference method. Must be odd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3f3db",
   "metadata": {},
   "source": [
    "## Problem 3 (25 Points)\n",
    "\n",
    "1. Use the `scipy.misc.derivative()` function with `order=3` to estimate the derivative of the $cos(x)$ and compute the difference relative to the analytical result (what you would get doing it by hand), and plot this difference. I give you a function to calculate the derivative from scratch using a centered method, plot the difference between this and the analytical result as well. Feel free to try changing some of the parameters like `dx` or `order` and see how they impact your result.\n",
    "\n",
    "1. Try a different value of `n` (the number of derivatives). Plot your estimates of `n`-th derivative. Does it behave like you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af03dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centered_dy(y, x):\n",
    "    '''\n",
    "        Uses centered differences (see below) to estimate the derivatives at each value of x, \n",
    "        except for the first and last values. The derivative at the first value of x is estimated \n",
    "        using a forward difference. The derivative at the last value of x is estimated \n",
    "        using a backward difference.\n",
    "            dy/dx at x[i] is approximated by (y[i+1] - y[i-1]) / (x[i+1]-x[i-1])\n",
    "    '''\n",
    "    dyc = [0.0]*len(x)\n",
    "    dyc[0] = (y[0] - y[1])/(x[0] - x[1])\n",
    "    for i in range(1,len(y)-1):\n",
    "        dyc[i] = (y[i+1] - y[i-1])/(x[i+1]-x[i-1])\n",
    "    dyc[-1] = (y[-1] - y[-2])/(x[-1] - x[-2])\n",
    "\n",
    "    return dyc\n",
    "\n",
    "###Your Code Here###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28eeaa1",
   "metadata": {},
   "source": [
    "# Numerical Integration\n",
    "\n",
    "## Problem 4 (25 Points)\n",
    "\n",
    "### Adapted from [Ayars 2.2] and Physics 77\n",
    "\n",
    "Compare results of the trapezoid integration method,  Simpson’s method, and the adaptive Gaussian quadrature method for the following integrals:\n",
    "\n",
    "1. $$\\int_0^{\\pi}\\sin x\\, dx $$\n",
    "1. $$\\int_2^4 (x^2+x+1)\\, dx$$\n",
    "\n",
    "For both parts, try it with more and with fewer slices to determine how many slices are required to give an ‘acceptable’ answer. (If you double the number of slices and still get the same answer, then try half as many, etc.) Part 2 is particularly interesting in this regard. In your submitted work, describe roughly how many points were required, and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59104d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Your Code Here###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
